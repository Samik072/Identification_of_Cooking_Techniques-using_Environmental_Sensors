{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBQPACFj-j41"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49Iu_tX5-vmy"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/Datasetfinal.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QC8fC6b-TLTR"
      },
      "outputs": [],
      "source": [
        "df.replace('$', np.nan, inplace = True)\n",
        "df.replace('$$', np.nan, inplace = True)\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'],format=\"%d-%m-%Y %H:%M:%S\")\n",
        "df['PM2.5(ug/m3)'] = df['PM2.5(ug/m3)'].astype('float64')\n",
        "df['PM10(ug/m3)'] = df['PM10(ug/m3)'].astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIrDB4-IuhVE"
      },
      "outputs": [],
      "source": [
        "df['Temp(C)'] = df['Temp(C)'].replace('$$', np.nan)  # Replace '$$' with NaN if needed\n",
        "df['Temp(C)'] = df['Temp(C)'].str.replace('C', '')  # Remove 'C' from all values\n",
        "df['Temp(C)'] = df['Temp(C)'].astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQIziU0zuhVE"
      },
      "outputs": [],
      "source": [
        "df['Humi(%)'] = df['Humi(%)'].replace('$$', np.nan)  # Replace '$$' with NaN if needed\n",
        "df['Humi(%)'] = df['Humi(%)'].str.replace('%', '')  # Remove '%' from all values\n",
        "df['Humi(%)'] = df['Humi(%)'].astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R1KZDgpTgwu",
        "outputId": "71d0bf9c-381f-4f04-8355-a227f3ce3431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target\n"
          ]
        }
      ],
      "source": [
        "# Find the columns which contain strings\n",
        "for column in df.columns:\n",
        "    if pd.api.types.is_string_dtype(df[column]):\n",
        "        print(column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH4wGJ7ITiyL",
        "outputId": "146d3bb1-5c5e-4bcc-903a-6c56ba706125"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Timestamp': False,\n",
              " 'Temp(C)': False,\n",
              " 'Humi(%)': False,\n",
              " 'VOC': False,\n",
              " 'PM1.0(ug/m3)': False,\n",
              " 'PM2.5(ug/m3)': False,\n",
              " 'PM10(ug/m3)': False,\n",
              " 'CO2': False,\n",
              " 'Target': True}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "{i: pd.api.types.is_string_dtype(df[i]) for i in df.columns}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsT1bqPsTktH"
      },
      "outputs": [],
      "source": [
        "for column in df.columns:\n",
        "    if pd.api.types.is_string_dtype(df[column]):\n",
        "        df[column] = df[column].astype('category').cat.as_ordered()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO53lLRrTm0i",
        "outputId": "eb8a739d-391f-4b1a-b1f9-0cbc42f20096"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Timestamp       0.000000\n",
              "Temp(C)         0.000000\n",
              "Humi(%)         0.000000\n",
              "VOC             0.000000\n",
              "PM1.0(ug/m3)    0.000000\n",
              "PM2.5(ug/m3)    0.000000\n",
              "PM10(ug/m3)     0.000000\n",
              "CO2             0.029218\n",
              "Target          0.000000\n",
              "dtype: float64"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check missing data ratio percentage\n",
        "\n",
        "df.isna().sum() * 100.00 / len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5EwdWqYTpZQ",
        "outputId": "bf554aaf-5f34-4663-8f14-bbbf6e675626"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Timestamp       0\n",
              "Temp(C)         0\n",
              "Humi(%)         0\n",
              "VOC             0\n",
              "PM1.0(ug/m3)    0\n",
              "PM2.5(ug/m3)    0\n",
              "PM10(ug/m3)     0\n",
              "CO2             2\n",
              "Target          0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6XsDC7ATraq",
        "outputId": "27f1559a-5316-4752-9fa3-32efc991379f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temp(C)\n",
            "Humi(%)\n",
            "VOC\n",
            "PM1.0(ug/m3)\n",
            "PM2.5(ug/m3)\n",
            "PM10(ug/m3)\n",
            "CO2\n"
          ]
        }
      ],
      "source": [
        "for column in df.columns:\n",
        "    if pd.api.types.is_numeric_dtype(df[column]):\n",
        "        print(column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJQNB4GgTuHj"
      },
      "outputs": [],
      "source": [
        "# Check for which numeric columns have null values\n",
        "for column in df.columns:\n",
        "    if pd.api.types.is_numeric_dtype(df[column]) and df[column].isna().sum():\n",
        "        print(column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqAgidVIuhVH"
      },
      "outputs": [],
      "source": [
        "# Check which rows have null values in any column\n",
        "null_rows_any_column = df[df.isnull().any(axis=1)]\n",
        "\n",
        "# Display the rows with null values in any column\n",
        "print(null_rows_any_column)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLlaLleRuhVH"
      },
      "outputs": [],
      "source": [
        "column_name = 'CO2'\n",
        "\n",
        "# Remove rows with null values in the specified column\n",
        "df = df.dropna(subset=[column_name])\n",
        "\n",
        "# If you want to remove rows with null values across all columns, you can use:\n",
        "# df = df.dropna()\n",
        "\n",
        "# Reset the index after removing rows\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Display the DataFrame after removing null values\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruNRhT0_Twqe"
      },
      "outputs": [],
      "source": [
        "ax = df['Target'].value_counts().plot(kind = 'bar')\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, fmt='%d', label_type='edge')\n",
        "\n",
        "plt.xticks(rotation = 90);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArZTGUfQTzWZ"
      },
      "outputs": [],
      "source": [
        "# Dropping all missing rows\n",
        "new_df = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFGY_wNU_eA1"
      },
      "outputs": [],
      "source": [
        "# Check for columns which aren't numeric or is categorical\n",
        "# and print category codes\n",
        "l = 0\n",
        "for column in new_df.columns:\n",
        "    if pd.api.types.is_categorical_dtype(new_df[column]):\n",
        "        l += 1\n",
        "        print(f'{column}: {dict(enumerate(df[column].cat.categories))}')\n",
        "print(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XionnzJT_S1"
      },
      "outputs": [],
      "source": [
        "# Turn all categorical variables into numbers and fill missing\n",
        "for column in new_df.columns:\n",
        "    if pd.api.types.is_categorical_dtype(new_df[column]):\n",
        "        # Turn categories into numbers and add +1\n",
        "        new_df[column] = pd.Categorical(new_df[column]).codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "JW5O_Bfk_uXj",
        "outputId": "e5daad61-2efc-4236-d811-56257c6cae78"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Temp(C)</th>\n",
              "      <th>Humi(%)</th>\n",
              "      <th>VOC</th>\n",
              "      <th>PM1.0(ug/m3)</th>\n",
              "      <th>PM2.5(ug/m3)</th>\n",
              "      <th>PM10(ug/m3)</th>\n",
              "      <th>CO2</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023-12-28 08:38:39</td>\n",
              "      <td>22.59</td>\n",
              "      <td>87.62</td>\n",
              "      <td>0</td>\n",
              "      <td>408</td>\n",
              "      <td>201.0</td>\n",
              "      <td>442.0</td>\n",
              "      <td>813.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-12-28 08:38:44</td>\n",
              "      <td>22.62</td>\n",
              "      <td>87.25</td>\n",
              "      <td>0</td>\n",
              "      <td>410</td>\n",
              "      <td>202.0</td>\n",
              "      <td>444.0</td>\n",
              "      <td>813.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-12-28 08:38:49</td>\n",
              "      <td>22.66</td>\n",
              "      <td>87.06</td>\n",
              "      <td>0</td>\n",
              "      <td>408</td>\n",
              "      <td>201.0</td>\n",
              "      <td>441.0</td>\n",
              "      <td>814.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-12-28 08:38:54</td>\n",
              "      <td>22.62</td>\n",
              "      <td>86.94</td>\n",
              "      <td>0</td>\n",
              "      <td>406</td>\n",
              "      <td>201.0</td>\n",
              "      <td>439.0</td>\n",
              "      <td>812.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-12-28 08:38:59</td>\n",
              "      <td>22.66</td>\n",
              "      <td>86.50</td>\n",
              "      <td>0</td>\n",
              "      <td>404</td>\n",
              "      <td>200.0</td>\n",
              "      <td>437.0</td>\n",
              "      <td>814.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Timestamp  Temp(C)  Humi(%)  VOC  PM1.0(ug/m3)  PM2.5(ug/m3)  \\\n",
              "0 2023-12-28 08:38:39    22.59    87.62    0           408         201.0   \n",
              "1 2023-12-28 08:38:44    22.62    87.25    0           410         202.0   \n",
              "2 2023-12-28 08:38:49    22.66    87.06    0           408         201.0   \n",
              "3 2023-12-28 08:38:54    22.62    86.94    0           406         201.0   \n",
              "4 2023-12-28 08:38:59    22.66    86.50    0           404         200.0   \n",
              "\n",
              "   PM10(ug/m3)    CO2  Target  \n",
              "0        442.0  813.0       5  \n",
              "1        444.0  813.0       5  \n",
              "2        441.0  814.0       5  \n",
              "3        439.0  812.0       5  \n",
              "4        437.0  814.0       5  "
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAmLiJDTAZF2"
      },
      "source": [
        "# ML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8BjBkdZvCig"
      },
      "source": [
        "## Original ML Pipeline Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBp0MjABvASb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
        "import time\n",
        "np.random.seed(42)\n",
        "\n",
        "class MultiModelEvaluator:\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "        self.model_names = list(models.keys())\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "        self.metric_scores = {}\n",
        "\n",
        "    def split_data(self, X, y, test_size=0.3, random_state=42):\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    def train_models(self):\n",
        "        for model_name, model in self.models.items():\n",
        "            print(f\"\\n================================================\\n{model_name} model has started training\")\n",
        "            start = time.time()\n",
        "            pipeline = Pipeline([\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('classifier', model)\n",
        "            ])\n",
        "            pipeline.fit(self.X_train, self.y_train)\n",
        "            self.models[model_name] = pipeline\n",
        "            print(f\"{model_name} model has ended training. Time -> {round(time.time() - start, 2)}s. Accuracy - > {(pipeline.score(self.X_test, self.y_test) * 100.00):.2f} %\\n================================================\\n\")\n",
        "\n",
        "    def evaluate_models(self, X_test, y_test):\n",
        "        for model_name, pipeline in self.models.items():\n",
        "            Train_y_pred = pipeline.predict(self.X_train)\n",
        "            Train_accuracy = accuracy_score(self.y_train, Train_y_pred)\n",
        "            Train_f1_macro = f1_score(self.y_train, Train_y_pred, average='macro')\n",
        "            Train_f1_weighted = f1_score(self.y_train, Train_y_pred, average='weighted')\n",
        "            Train_recall_macro = recall_score(self.y_train, Train_y_pred, average='macro')\n",
        "            Train_recall_weighted = recall_score(self.y_train, Train_y_pred, average='weighted')\n",
        "            Train_precision_macro = precision_score(self.y_train, Train_y_pred, average='macro')\n",
        "            Train_precision_weighted = precision_score(self.y_train, Train_y_pred, average='weighted')\n",
        "            Train_confusion = confusion_matrix(self.y_train, Train_y_pred)\n",
        "\n",
        "            Test_y_pred = pipeline.predict(X_test)\n",
        "            Test_accuracy = accuracy_score(y_test, Test_y_pred)\n",
        "            Test_f1_macro = f1_score(y_test, Test_y_pred, average='macro')\n",
        "            Test_f1_weighted = f1_score(y_test, Test_y_pred, average='weighted')\n",
        "            Test_recall_macro = recall_score(y_test, Test_y_pred, average='macro')\n",
        "            Test_recall_weighted = recall_score(y_test, Test_y_pred, average='weighted')\n",
        "            Test_precision_macro = precision_score(y_test, Test_y_pred, average='macro')\n",
        "            Test_precision_weighted = precision_score(y_test, Test_y_pred, average='weighted')\n",
        "            Test_confusion = confusion_matrix(y_test, Test_y_pred)\n",
        "\n",
        "            self.metric_scores[model_name] = {\n",
        "                'Train Accuracy': Train_accuracy,\n",
        "                'Train F1 Macro': Train_f1_macro,\n",
        "                'Train F1 Weighted': Train_f1_weighted,\n",
        "                'Train Recall Macro': Train_recall_macro,\n",
        "                'Train Recall Weighted': Train_recall_weighted,\n",
        "                'Train Precision Macro': Train_precision_macro,\n",
        "                'Train Precision Weighted': Train_precision_weighted,\n",
        "                'Train Confusion Matrix': Train_confusion,\n",
        "                'Test Accuracy': Test_accuracy,\n",
        "                'Test F1 Macro': Test_f1_macro,\n",
        "                'Test F1 Weighted': Test_f1_weighted,\n",
        "                'Test Recall Macro': Test_recall_macro,\n",
        "                'Test Recall Weighted': Test_recall_weighted,\n",
        "                'Test Precision Macro': Test_precision_macro,\n",
        "                'Test Precision Weighted': Test_precision_weighted,\n",
        "                'Test Confusion Matrix': Test_confusion\n",
        "            }\n",
        "\n",
        "    def get_metric_scores(self, model_name):\n",
        "        return self.metric_scores.get(model_name, {})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woI-Nyg9vOb3"
      },
      "source": [
        "## Hyper-Tuned Pipeline Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6h1KYc9vOEn"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "class MultiModelEvaluatorWithTuning:\n",
        "    def __init__(self, models, param_grids, n_iter_values = {}, n_jobs_values = {}, verbose_values = {}):\n",
        "        self.models = models\n",
        "        self.model_names = list(models.keys())\n",
        "        self.param_grids = param_grids\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "        self.n_iter_values = n_iter_values\n",
        "        self.n_jobs_values = n_jobs_values\n",
        "        self.verbose_values = verbose_values\n",
        "        self.metric_scores = {}\n",
        "        self.best_params = {}\n",
        "\n",
        "    def split_data(self, X, y, test_size=0.3, random_state=42):\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    def train_models(self):\n",
        "        for model_name, model in self.models.items():\n",
        "            print(f\"\\n================================================\\n{model_name} tuned model has started training\")\n",
        "            start = time.time()\n",
        "            pipeline = Pipeline([\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('classifier', model)\n",
        "            ])\n",
        "            if model_name not in self.n_iter_values:\n",
        "                search = GridSearchCV(pipeline, self.param_grids[model_name], cv=5, n_jobs = self.n_jobs_values.get(model_name, -1), verbose = self.verbose_values.get(model_name, 1))\n",
        "            else:\n",
        "                search = RandomizedSearchCV(pipeline, self.param_grids[model_name], cv=5, n_jobs = self.n_jobs_values.get(model_name, -1), n_iter = self.n_iter_values.get(model_name, 10), verbose = self.verbose_values.get(model_name, 1))\n",
        "            search.fit(self.X_train, self.y_train)\n",
        "            best_model = search.best_estimator_\n",
        "            self.models[model_name] = best_model\n",
        "            self.best_params[model_name] = search.best_params_\n",
        "            print(f\"{model_name} tuned model has ended training. Time -> {round(time.time() - start, 2)}s. Accuracy - > {(best_model.score(self.X_test, self.y_test) * 100.00):.2f} %\\n================================================\\n\")\n",
        "\n",
        "\n",
        "    def evaluate_models(self, X_test, y_test):\n",
        "        for model_name, pipeline in self.models.items():\n",
        "            Train_y_pred = pipeline.predict(self.X_train)\n",
        "            Train_accuracy = accuracy_score(self.y_train, Train_y_pred)\n",
        "            Train_f1_macro = f1_score(self.y_train, Train_y_pred, average='macro')\n",
        "            Train_f1_weighted = f1_score(self.y_train, Train_y_pred, average='weighted')\n",
        "            Train_recall_macro = recall_score(self.y_train, Train_y_pred, average='macro')\n",
        "            Train_recall_weighted = recall_score(self.y_train, Train_y_pred, average='weighted')\n",
        "            Train_precision_macro = precision_score(self.y_train, Train_y_pred, average='macro')\n",
        "            Train_precision_weighted = precision_score(self.y_train, Train_y_pred, average='weighted')\n",
        "            Train_confusion = confusion_matrix(self.y_train, Train_y_pred)\n",
        "\n",
        "            Test_y_pred = pipeline.predict(X_test)\n",
        "            Test_accuracy = accuracy_score(y_test, Test_y_pred)\n",
        "            Test_f1_macro = f1_score(y_test, Test_y_pred, average='macro')\n",
        "            Test_f1_weighted = f1_score(y_test, Test_y_pred, average='weighted')\n",
        "            Test_recall_macro = recall_score(y_test, Test_y_pred, average='macro')\n",
        "            Test_recall_weighted = recall_score(y_test, Test_y_pred, average='weighted')\n",
        "            Test_precision_macro = precision_score(y_test, Test_y_pred, average='macro')\n",
        "            Test_precision_weighted = precision_score(y_test, Test_y_pred, average='weighted')\n",
        "            Test_confusion = confusion_matrix(y_test, Test_y_pred)\n",
        "\n",
        "            self.metric_scores[model_name] = {\n",
        "                'Train Accuracy': Train_accuracy,\n",
        "                'Train F1 Macro': Train_f1_macro,\n",
        "                'Train F1 Weighted': Train_f1_weighted,\n",
        "                'Train Recall Macro': Train_recall_macro,\n",
        "                'Train Recall Weighted': Train_recall_weighted,\n",
        "                'Train Precision Macro': Train_precision_macro,\n",
        "                'Train Precision Weighted': Train_precision_weighted,\n",
        "                'Train Confusion Matrix': Train_confusion,\n",
        "                'Test Accuracy': Test_accuracy,\n",
        "                'Test F1 Macro': Test_f1_macro,\n",
        "                'Test F1 Weighted': Test_f1_weighted,\n",
        "                'Test Recall Macro': Test_recall_macro,\n",
        "                'Test Recall Weighted': Test_recall_weighted,\n",
        "                'Test Precision Macro': Test_precision_macro,\n",
        "                'Test Precision Weighted': Test_precision_weighted,\n",
        "                'Test Confusion Matrix': Test_confusion\n",
        "            }\n",
        "\n",
        "    def get_metric_scores(self, model_name):\n",
        "        return self.metric_scores.get(model_name, {})\n",
        "\n",
        "    def get_best_params(self, model_name):\n",
        "        return self.best_params.get(model_name, {})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class DeepModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        super(DeepModel, self).__init__()\n",
        "        layers = []\n",
        "        in_features = input_size\n",
        "\n",
        "        # Input normalization\n",
        "        layers.append(nn.BatchNorm1d(input_size))\n",
        "\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(in_features, hidden_size))\n",
        "            # Batch normalization for hidden layers\n",
        "            layers.append(nn.BatchNorm1d(hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            in_features = hidden_size\n",
        "\n",
        "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "def evaluate_accuracy(model, data_loader):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "    accuracy = total_correct / total_samples\n",
        "    return accuracy\n",
        "\n",
        "def train_and_evaluate(model, train_loader, dev_loader, y_fold_dev_list):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "    num_epochs = 50\n",
        "\n",
        "    # Lists to store convergence data\n",
        "    train_losses = []\n",
        "    dev_losses = []\n",
        "    train_accuracies = []\n",
        "    dev_accuracies = []\n",
        "\n",
        "    # Lists to store true labels for each fold\n",
        "    true_labels_list = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Compute and store losses and accuracies\n",
        "        train_loss = loss.item()\n",
        "        train_losses.append(train_loss)\n",
        "        train_acc = evaluate_accuracy(model, train_loader)\n",
        "        train_accuracies.append(train_acc)\n",
        "        dev_acc = evaluate_accuracy(model, dev_loader)\n",
        "        dev_accuracies.append(dev_acc)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Dev Accuracy: {dev_acc:.4f}\")\n",
        "\n",
        "    # Plot convergence graph\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train Accuracy')\n",
        "    plt.plot(range(1, num_epochs + 1), dev_accuracies, label='Dev Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    predicted_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dev_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "            predicted_labels.extend(predicted.tolist())\n",
        "\n",
        "            # Store true labels for the current fold\n",
        "            true_labels_list.extend(labels.tolist())\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "\n",
        "    # Classification report and F1-score\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(true_labels_list, predicted_labels))\n",
        "    f1 = f1_score(true_labels_list, predicted_labels, average='weighted')\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "def ann_model(X, y, num_folds=5, test_size=0.3):\n",
        "    # Convert to PyTorch tensors\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    # Split the data into training, validation, and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "    # Initialize the model\n",
        "    model = DeepModel(input_size=X.shape[1], hidden_sizes=[64, 32, 16, 8], output_size=y.shape[0])\n",
        "\n",
        "    # Perform k-fold cross-validation on the training set\n",
        "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    best_model = None\n",
        "\n",
        "    for fold, (train_index, dev_index) in enumerate(kf.split(X_train)):\n",
        "        X_fold_train, X_fold_dev = X_train[train_index], X_train[dev_index]\n",
        "        y_fold_train, y_fold_dev = y_train[train_index], y_train[dev_index]\n",
        "\n",
        "        train_loader = data.DataLoader(data.TensorDataset(X_fold_train, y_fold_train), batch_size=32, shuffle=True)\n",
        "        dev_loader = data.DataLoader(data.TensorDataset(X_fold_dev, y_fold_dev), batch_size=32, shuffle=False)\n",
        "\n",
        "        # Train the model\n",
        "        accuracy = train_and_evaluate(model, train_loader, dev_loader, y_fold_dev_list=y_fold_dev)\n",
        "\n",
        "        print(f\"Fold {fold + 1}/{num_folds}, Accuracy: {accuracy}\")\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_model = model\n",
        "\n",
        "    print(f\"Best model, Best accuracy: {best_accuracy}\")\n",
        "\n",
        "    # Evaluate the best model on the separate test set\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "    test_loader = data.DataLoader(data.TensorDataset(X_test, y_test), batch_size=32, shuffle=False)\n",
        "\n",
        "    test_accuracy = evaluate_accuracy(best_model, test_loader)\n",
        "    print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "    return best_model\n",
        "\n",
        "# Call ann_model with your data\n",
        "# ann_model(X_data, y_data, num_folds=5, test_size=0.3)\n"
      ],
      "metadata": {
        "id": "48ROGsiL4Kw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0QAoqBh07SI"
      },
      "outputs": [],
      "source": [
        "def original_ml_pipeline_obj(x, y, test_size = 0.3):\n",
        "    # Define the machine learning models\n",
        "    models = {\n",
        "\n",
        "        'SVM': SVC(kernel = 'rbf', gamma = 0.1, C = 1.0),\n",
        "        'MLP Neural Net': MLPClassifier(alpha=1, max_iter=1000, random_state=42),\n",
        "    }\n",
        "\n",
        "    # Initialize the MultiModelEvaluator\n",
        "    evaluator = MultiModelEvaluator(models)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    evaluator.split_data(x, y, test_size = test_size)\n",
        "\n",
        "    # Train the models\n",
        "    evaluator.train_models()\n",
        "\n",
        "    # Evaluate the models\n",
        "    evaluator.evaluate_models(evaluator.X_test, evaluator.y_test)\n",
        "\n",
        "    # Get metric scores for a specific model\n",
        "    # model_name = 'RandomForest'\n",
        "    # scores = evaluator.get_metric_scores(model_name)\n",
        "    # print(f'Metric Scores for Model {model_name}:')\n",
        "    # for metric, score in scores.items():\n",
        "    #     print(f'{metric}: {score}')\n",
        "    return evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYM8S-cG1aUV"
      },
      "outputs": [],
      "source": [
        "def hyper_tuned_ml_pipeline_obj(x, y, test_size = 0.3):\n",
        "    # Define the machine learning models\n",
        "    models = {\n",
        "        'SVM': SVC(kernel = 'rbf', gamma = 0.1, C = 1.0),\n",
        "        'MLP Neural Net': MLPClassifier(alpha=1, max_iter=1000, random_state=42),\n",
        "    }\n",
        "\n",
        "    n_jobs_values = {\n",
        "\n",
        "    }\n",
        "\n",
        "    verbose_values = {\n",
        "\n",
        "    }\n",
        "\n",
        "    n_iter_values = {\n",
        "\n",
        "        'MLP Neural Net': 50,\n",
        "    }\n",
        "\n",
        "    # Define parameter grids for hyperparameter tuning\n",
        "    param_grids = {\n",
        "\n",
        "\n",
        "        'SVM': {'classifier__C': [0.1, 0.5, 1, 2, 5, 10, 20], 'classifier__kernel': ['rbf'], \"classifier__gamma\": [0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1]},\n",
        "\n",
        "\n",
        "        'MLP Neural Net': {'classifier__hidden_layer_sizes': [(150,100,50), (120,80,40), (100,50,30)], 'classifier__max_iter': [50, 100, 150],\n",
        "                           'classifier__activation': ['tanh', 'relu'], 'classifier__solver': ['lbfgs', 'adam'], 'classifier__alpha': [0.0001, 0.05],\n",
        "                           'classifier__learning_rate': ['constant','adaptive'], 'classifier__random_state': [42]},\n",
        "\n",
        "        }\n",
        "\n",
        "    # Initialize the MultiModelEvaluatorWithTuning\n",
        "    evaluator = MultiModelEvaluatorWithTuning(models, param_grids, n_iter_values = n_iter_values, n_jobs_values = n_jobs_values, verbose_values = verbose_values)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    evaluator.split_data(x, y, test_size = test_size)\n",
        "\n",
        "    # Train the models with hyperparameter tuning\n",
        "    evaluator.train_models()\n",
        "\n",
        "    # Evaluate the models\n",
        "    evaluator.evaluate_models(evaluator.X_test, evaluator.y_test)\n",
        "\n",
        "    # Get metric scores for a specific model\n",
        "    # model_name = 'RandomForest'\n",
        "    # scores = evaluator_tuned.get_metric_scores(model_name)\n",
        "    # print(f'Metric Scores for Model {model_name}:')\n",
        "    # for metric, score in scores.items():\n",
        "    #     print(f'{metric}: {score}')\n",
        "\n",
        "    return evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuAlp7h4xUBH"
      },
      "outputs": [],
      "source": [
        "def evaluate_result(evaluator):\n",
        "    result = {'Model Name': [], 'Train Accuracy': [], 'Train F1 Macro': [], 'Train F1 Weighted': [], 'Train Recall Macro': [],\n",
        "              'Train Recall Weighted': [], 'Train Precision Macro': [], 'Train Precision Weighted': [],\n",
        "              'Test Accuracy': [], 'Test F1 Macro': [], 'Test F1 Weighted': [], 'Test Recall Macro': [],\n",
        "              'Test Recall Weighted': [], 'Test Precision Macro': [], 'Test Precision Weighted': []\n",
        "              }\n",
        "\n",
        "    for model_name in evaluator.metric_scores:\n",
        "        result['Model Name'].append(model_name)\n",
        "        for metric_name in list(result.keys())[1:]:\n",
        "            result[metric_name].append(round(evaluator.metric_scores[model_name][metric_name] * 100.00, 2))\n",
        "\n",
        "    result = pd.DataFrame(result).T\n",
        "    result.rename(columns=result.iloc[0], inplace = True)\n",
        "    result.drop(result.index[0], inplace = True)\n",
        "    result.index = pd.MultiIndex.from_tuples(\n",
        "        [('Train', metric.replace('Train ', '')) if i < 7 else ('Test', metric.replace('Test ', '')) for i, metric in enumerate(result.index)],\n",
        "        names=['', 'Metrics'])\n",
        "    result.index.names = ['',  'Metrics']\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUibUpDC4ZsO"
      },
      "outputs": [],
      "source": [
        "def evaluate_ann(model, X_train, X_test, y_train, y_test):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Train_y_pred = torch.max(model(torch.tensor(X_train, dtype=torch.float32)), 1)[1].numpy()\n",
        "    Train_accuracy = accuracy_score(y_train, Train_y_pred)\n",
        "    Train_f1_macro = f1_score(y_train, Train_y_pred, average='macro')\n",
        "    Train_f1_weighted = f1_score(y_train, Train_y_pred, average='weighted')\n",
        "    Train_recall_macro = recall_score(y_train, Train_y_pred, average='macro')\n",
        "    Train_recall_weighted = recall_score(y_train, Train_y_pred, average='weighted')\n",
        "    Train_precision_macro = precision_score(y_train, Train_y_pred, average='macro')\n",
        "    Train_precision_weighted = precision_score(y_train, Train_y_pred, average='weighted')\n",
        "    Train_confusion = confusion_matrix(y_train, Train_y_pred)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Test_y_pred = torch.max(model(torch.tensor(X_test, dtype=torch.float32)), 1)[1].numpy()\n",
        "    Test_accuracy = accuracy_score(y_test, Test_y_pred)\n",
        "    Test_f1_macro = f1_score(y_test, Test_y_pred, average='macro')\n",
        "    Test_f1_weighted = f1_score(y_test, Test_y_pred, average='weighted')\n",
        "    Test_recall_macro = recall_score(y_test, Test_y_pred, average='macro')\n",
        "    Test_recall_weighted = recall_score(y_test, Test_y_pred, average='weighted')\n",
        "    Test_precision_macro = precision_score(y_test, Test_y_pred, average='macro')\n",
        "    Test_precision_weighted = precision_score(y_test, Test_y_pred, average='weighted')\n",
        "    Test_confusion = confusion_matrix(y_test, Test_y_pred)\n",
        "\n",
        "    metrics = {\n",
        "                    'Train Accuracy': Train_accuracy,\n",
        "                    'Train F1 Macro': Train_f1_macro,\n",
        "                    'Train F1 Weighted': Train_f1_weighted,\n",
        "                    'Train Recall Macro': Train_recall_macro,\n",
        "                    'Train Recall Weighted': Train_recall_weighted,\n",
        "                    'Train Precision Macro': Train_precision_macro,\n",
        "                    'Train Precision Weighted': Train_precision_weighted,\n",
        "                    'Train Confusion Matrix': Train_confusion,\n",
        "                    'Test Accuracy': Test_accuracy,\n",
        "                    'Test F1 Macro': Test_f1_macro,\n",
        "                    'Test F1 Weighted': Test_f1_weighted,\n",
        "                    'Test Recall Macro': Test_recall_macro,\n",
        "                    'Test Recall Weighted': Test_recall_weighted,\n",
        "                    'Test Precision Macro': Test_precision_macro,\n",
        "                    'Test Precision Weighted': Test_precision_weighted,\n",
        "                    'Test Confusion Matrix': Test_confusion\n",
        "                }\n",
        "\n",
        "    result = {'Model Name': [], 'Train Accuracy': [], 'Train F1 Macro': [], 'Train F1 Weighted': [], 'Train Recall Macro': [],\n",
        "              'Train Recall Weighted': [], 'Train Precision Macro': [], 'Train Precision Weighted': [],\n",
        "              'Test Accuracy': [], 'Test F1 Macro': [], 'Test F1 Weighted': [], 'Test Recall Macro': [],\n",
        "              'Test Recall Weighted': [], 'Test Precision Macro': [], 'Test Precision Weighted': []\n",
        "              }\n",
        "    result['Model Name'].append('ANN')\n",
        "    for metric_name in list(result.keys())[1:]:\n",
        "        result[metric_name].append(round(metrics[metric_name] * 100.00, 2))\n",
        "\n",
        "    result = pd.DataFrame(result).T\n",
        "    result.rename(columns=result.iloc[0], inplace = True)\n",
        "    result.drop(result.index[0], inplace = True)\n",
        "    result.index = pd.MultiIndex.from_tuples(\n",
        "        [('Train', metric.replace('Train ', '')) if i < 7 else ('Test', metric.replace('Test ', '')) for i, metric in enumerate(result.index)],\n",
        "        names=['', 'Metrics'])\n",
        "    result.index.names = ['',  'Metrics']\n",
        "    return metrics, result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aln2Pyn4WCF"
      },
      "outputs": [],
      "source": [
        "def plot_feature_importances(models, model_names, feature_names):\n",
        "    \"\"\"\n",
        "    Plot feature importances for a list of machine learning models.\n",
        "\n",
        "    Parameters:\n",
        "    - models (list): List of trained models.\n",
        "    - model_names (list): Names of the models for labeling in the plot.\n",
        "    - feature_names (list): Names of the features for labeling in the plot.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    num_models = len(models)\n",
        "    num_features = len(feature_names)\n",
        "\n",
        "    for i in range(num_models):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        if isinstance(models[i], DecisionTreeClassifier) or isinstance(models[i], RandomForestClassifier):\n",
        "            importances = models[i].feature_importances_\n",
        "        elif isinstance(models[i], XGBClassifier) or isinstance(models[i], AdaBoostClassifier):\n",
        "            importances = models[i].feature_importances_\n",
        "        elif isinstance(models[i], LGBMClassifier):\n",
        "            importances = models[i].feature_importances_\n",
        "        elif isinstance(models[i], LogisticRegression):\n",
        "            importances = np.abs(models[i].coef_[0])\n",
        "        elif isinstance(models[i], MLPClassifier):\n",
        "            importances = [np.sum(np.abs(layer), axis=1) for layer in [layer / np.linalg.norm(layer, ord=2, axis=0)\n",
        "                            for layer in models[i].coefs_]][0]\n",
        "        # elif isinstance(models[i], KNeighborsClassifier) or isinstance(models[i], SVC):\n",
        "        #     importances = [1] * num_features  # KNN and SVM doesn't have feature importance, for instance let's say every feature importance is equal\n",
        "        elif isinstance(models[i], GradientBoostingClassifier):\n",
        "            importances = models[i].feature_importances_\n",
        "        else:\n",
        "            pass  # KNN, SVM, Naive Bayes and QDA doesn't have feature importance\n",
        "            # raise ValueError(f\"Unsupported model type: {type(models[i])}\")\n",
        "\n",
        "        # Sort feature importances in descending order\n",
        "        sorted_indices = np.argsort(importances)[::-1]\n",
        "        sorted_importances = [importances[idx] for idx in sorted_indices]  # Convert to a list of values\n",
        "        sorted_feature_names = [feature_names[idx] for idx in sorted_indices]\n",
        "\n",
        "        plt.bar(range(num_features), sorted_importances, tick_label=sorted_feature_names)\n",
        "        plt.title(f'Feature Importances for {model_names[i]}')\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx6P1vTRvhem"
      },
      "source": [
        "## ML Machine Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0el3YpJQyxC"
      },
      "outputs": [],
      "source": [
        "# for feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "st_x = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5M_eDMVAlpo"
      },
      "outputs": [],
      "source": [
        "# Preapring data for ML\n",
        "df_tmp = new_df.drop([\"Timestamp\"], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG5NdN4dApNn"
      },
      "outputs": [],
      "source": [
        "# Split data into x and y\n",
        "x = df_tmp.drop('Target', axis = 1)\n",
        "y = df_tmp['Target'].values  # converting to numpy array\n",
        "\n",
        "# Scaling input variables, output variables doens't required as we are just predicting discrete outcomes\n",
        "x = st_x.fit_transform(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2ftiRPDEGku"
      },
      "outputs": [],
      "source": [
        "evaluator = original_ml_pipeline_obj(x, y, test_size = 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GKvDry-9xPd"
      },
      "outputs": [],
      "source": [
        "result = evaluate_result(evaluator)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MECrX-aBF1u1"
      },
      "outputs": [],
      "source": [
        "metric_to_show = 'Accuracy'\n",
        "\n",
        "ax = result.loc[[('Train', metric_to_show), ('Test', metric_to_show)]].T.plot(marker='o', figsize=(14, 8))\n",
        "plt.title(f'Train and Test {metric_to_show} for Different Models')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel(metric_to_show)\n",
        "plt.xticks(np.arange(len(result.columns.to_numpy())), result.columns.to_numpy(), rotation=90)\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OIFs9Af5LxdZ"
      },
      "outputs": [],
      "source": [
        "evaluator_hyper_tuned = hyper_tuned_ml_pipeline_obj(x, y, test_size = 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtVpojhCcmsz"
      },
      "outputs": [],
      "source": [
        "result_hyper_tuned = evaluate_result(evaluator_hyper_tuned)\n",
        "result_hyper_tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-m9JkvHzB5DX"
      },
      "outputs": [],
      "source": [
        "metric_to_show = 'Accuracy'\n",
        "\n",
        "result_hyper_tuned.loc[[('Train', metric_to_show), ('Test', metric_to_show)]].T.plot(marker='o', figsize=(14, 8))\n",
        "plt.title(f'Train and Test {metric_to_show} for Different Hyper Tuned Models')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel(metric_to_show)\n",
        "plt.xticks(np.arange(len(result_hyper_tuned.columns.to_numpy())), result_hyper_tuned.columns.to_numpy(), rotation=90)\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlWjLtyT_-9Z"
      },
      "outputs": [],
      "source": [
        "ann_model = ann_model(x, y, test_size = 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUNBOkKbAAFm"
      },
      "outputs": [],
      "source": [
        "evaluate_ann(ann_model, *train_test_split(x, y, test_size=0.3, random_state=42))[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUvypSMiHCgd"
      },
      "outputs": [],
      "source": [
        "metric_to_show = 'Accuracy'       # Accuracy, F1 Macro, F1 Weighted, Recall Macro, Recall Weighted, Precision Macro, Precision Weighted\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plot the first set of data\n",
        "# result_hyper_tuned.loc[[('Train', metric_to_show), ('Test', metric_to_show)]].T.plot(ax=ax, marker='o', figsize=(14, 8), title=f'Train and Test {metric_to_show} for Different Models')\n",
        "result_hyper_tuned.loc[[('Test', metric_to_show)]].T.plot(ax=ax, marker='o', figsize=(14, 8), title=f'Test {metric_to_show} for Different Models')\n",
        "\n",
        "# Plot the second set of data\n",
        "# result.loc[[('Train', metric_to_show), ('Test', metric_to_show)]].T.plot(ax=ax, marker='o', figsize=(14, 8))\n",
        "result.loc[[('Test', metric_to_show)]].T.plot(ax=ax, marker='o', figsize=(14, 8))\n",
        "\n",
        "# Set labels, legends, and show the plot\n",
        "ax.set_xlabel('Model')\n",
        "ax.set_ylabel(metric_to_show)\n",
        "ax.grid(True)\n",
        "# ax.legend(['Hyper Tuned Train', 'Hyper Tuned Test', 'Original Train', 'Original Test'])\n",
        "ax.legend(['Hyper Tuned Test', 'Original Test'])\n",
        "plt.xticks(np.arange(len(result.columns.to_numpy())), result.columns.to_numpy(), rotation=90)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C60uzwjxewA"
      },
      "outputs": [],
      "source": [
        "model_names, models = [], []\n",
        "for model_name in evaluator_hyper_tuned.model_names:\n",
        "    model_names.append(model_name)\n",
        "    models.append(evaluator_hyper_tuned.models[model_name].named_steps['classifier'])\n",
        "plot_feature_importances(models, model_names, df_tmp.columns.to_numpy()[:-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIXOGmGD3aMc"
      },
      "source": [
        "Objects ->\n",
        "- evaluator\n",
        "- evaluator_hyper_tuned\n",
        "\n",
        "Results ->\n",
        "- result\n",
        "- result_hyper_tuned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAHs5yV5FPDN"
      },
      "source": [
        "# Saving ML models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBcV8WNHDU3o"
      },
      "outputs": [],
      "source": [
        "from joblib import Parallel, delayed\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIXQwnZAEr3y"
      },
      "outputs": [],
      "source": [
        "with open(\"original_models.pkl\", \"wb\") as file:\n",
        "    joblib.dump(evaluator, file)\n",
        "\n",
        "with open(\"hypertuned_models.pkl\", \"wb\") as file:\n",
        "    joblib.dump(evaluator_hyper_tuned, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Piw1DFeLBSth"
      },
      "outputs": [],
      "source": [
        "with open(\"ann_model.pkl\", \"wb\") as file:\n",
        "    torch.save(ann_model, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6Ge4M_1KcBi"
      },
      "outputs": [],
      "source": [
        "# checking\n",
        "\n",
        "with open(\"/content/hypertuned_models.pkl\", \"rb\") as file:\n",
        "    tmp = joblib.load(file)\n",
        "\n",
        "model_name = 'MLP Neural Net'\n",
        "scores = tmp.get_metric_scores(model_name)\n",
        "print(f'Metric Scores for Model {model_name}:')\n",
        "for metric, score in scores.items():\n",
        "    print(f'{metric}: {score}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdPBNtaqFGCb"
      },
      "outputs": [],
      "source": [
        "# checking\n",
        "ob = torch.load(\"/content/ann_models.pkl\")\n",
        "evaluate_ann(ob, *train_test_split(x, y, test_size=0.3, random_state=42))[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4O41fTuLA_z"
      },
      "source": [
        "## Merging ANN with other models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Cnm0oN7KRRJ"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/original_models.pkl\", \"rb\") as file:\n",
        "    tmp1 = joblib.load(file)\n",
        "\n",
        "tmp1 = evaluate_result(tmp1)\n",
        "\n",
        "ob = torch.load(\"/content/ann_models.pkl\")\n",
        "tmp2 = evaluate_ann(ob, *train_test_split(x, y, test_size=0.3, random_state=42))[1]\n",
        "\n",
        "\n",
        "tmp1.join(tmp2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4bWXXVoK_8h"
      },
      "outputs": [],
      "source": [
        "metric_to_show = 'Accuracy'\n",
        "\n",
        "tmp1.join(tmp2).loc[[('Train', metric_to_show), ('Test', metric_to_show)]].T.plot(marker='o', figsize=(14, 8))\n",
        "plt.title(f'Train and Test {metric_to_show} for Different Models')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel(metric_to_show)\n",
        "plt.xticks(np.arange(len(tmp1.join(tmp2).columns.to_numpy())), tmp1.join(tmp2).columns.to_numpy(), rotation=90)\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwjCz9BJLcsd"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/hypertuned_models.pkl\", \"rb\") as file:\n",
        "    tmp3 = joblib.load(file)\n",
        "\n",
        "tmp3 = evaluate_result(tmp3)\n",
        "\n",
        "ob = torch.load(\"/content/ann_models.pkl\")\n",
        "tmp4 = evaluate_ann(ob, *train_test_split(x, y, test_size=0.3, random_state=42))[1]\n",
        "\n",
        "\n",
        "tmp3.join(tmp4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsvKLuiHLxvM"
      },
      "outputs": [],
      "source": [
        "metric_to_show = 'Accuracy'\n",
        "\n",
        "tmp3.join(tmp4).loc[[('Train', metric_to_show), ('Test', metric_to_show)]].T.plot(marker='o', figsize=(14, 8))\n",
        "plt.title(f'Train and Test {metric_to_show} for Different Hyper Tuned Models')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel(metric_to_show)\n",
        "plt.xticks(np.arange(len(tmp3.join(tmp4).columns.to_numpy())), tmp3.join(tmp4).columns.to_numpy(), rotation=90)\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7Mv5CV5mrgb"
      },
      "source": [
        "#### FOR RESEARCH PAPER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8DHVR8BHyz7"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/original_models.pkl\", \"rb\") as file:\n",
        "    tmp1 = joblib.load(file)\n",
        "\n",
        "tmp1 = evaluate_result(tmp1)\n",
        "\n",
        "ob = torch.load(\"/content/ann_models.pkl\")\n",
        "tmp2 = evaluate_ann(ob, *train_test_split(x, y, test_size=0.3, random_state=42))[1]\n",
        "\n",
        "qwe = tmp1.join(tmp2).loc['Test'].loc[['Accuracy', 'F1 Weighted', 'Recall Weighted', 'Precision Weighted']][[ 'SVM','MLP Neural Net', 'ANN']].T\n",
        "qwe['F1 Weighted'] = qwe['F1 Weighted'].map(lambda x: round(x / 100, 2))\n",
        "qwe['Recall Weighted'] = qwe['Recall Weighted'].map(lambda x: round(x / 100, 2))\n",
        "qwe['Precision Weighted'] = qwe['Precision Weighted'].map(lambda x: round(x / 100, 2))\n",
        "qwe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWkS71c8iKAm"
      },
      "outputs": [],
      "source": [
        "print(qwe.to_markdown())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP5vFbODnDh-"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/hypertuned_models.pkl\", \"rb\") as file:\n",
        "    tmp3 = joblib.load(file)\n",
        "\n",
        "tmp3 = evaluate_result(tmp3)\n",
        "\n",
        "ob = torch.load(\"/content/ann_models.pkl\")\n",
        "tmp4 = evaluate_ann(ob, *train_test_split(x, y, test_size=0.3, random_state=42))[1]\n",
        "\n",
        "qwe_h = tmp3.join(tmp4).loc['Test'].loc[['Accuracy', 'F1 Weighted', 'Recall Weighted', 'Precision Weighted']][['KNN', 'SVM', 'DecisionTree', 'RandomForest', 'XGB', 'LGBM', 'MLP Neural Net', 'ANN']].T\n",
        "qwe_h['F1 Weighted'] = qwe_h['F1 Weighted'].map(lambda x: round(x / 100, 2))\n",
        "qwe_h['Recall Weighted'] = qwe_h['Recall Weighted'].map(lambda x: round(x / 100, 2))\n",
        "qwe_h['Precision Weighted'] = qwe_h['Precision Weighted'].map(lambda x: round(x / 100, 2))\n",
        "qwe_h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxFiwWhcw_lh"
      },
      "outputs": [],
      "source": [
        "print(qwe_h.to_markdown())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTCxmI_UnL0A"
      },
      "outputs": [],
      "source": [
        "# Sample data\n",
        "models = qwe.index.tolist()\n",
        "accuracy_nontuned = qwe['Accuracy']\n",
        "accuracy_tuned = qwe_h['Accuracy']\n",
        "\n",
        "# Set the figure size\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "bar_width = 0.35\n",
        "index = np.arange(len(models))\n",
        "\n",
        "# Plotting the bars\n",
        "bars1 = ax.bar(index, accuracy_nontuned, width=bar_width, label='Non-Tuned')\n",
        "bars2 = ax.bar(index + bar_width, accuracy_tuned, width=bar_width, label='Tuned')\n",
        "\n",
        "# Annotate the bars with accuracy values\n",
        "for bar, acc in zip(bars1, accuracy_nontuned):\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{int(acc)}%', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                xytext=(0, 3),  # 3 points vertical offset\n",
        "                textcoords=\"offset points\",\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "for bar, acc in zip(bars2, accuracy_tuned):\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{int(acc)}%', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                xytext=(0, 3),  # 3 points vertical offset\n",
        "                textcoords=\"offset points\",\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Models', fontsize=15)\n",
        "ax.set_ylabel('Accuracy (%)', fontsize=15)\n",
        "# ax.set_title('Comparison of Non-Tuned vs. Tuned Models', fontsize=15)\n",
        "\n",
        "# Rotate x-axis labels for better readability\n",
        "ax.set_xticks(index + bar_width / 2)\n",
        "ax.set_xticklabels(models, rotation=90, fontsize=15)\n",
        "\n",
        "ax.tick_params(axis='y', which='both', labelsize=15)\n",
        "# Add legend\n",
        "ax.legend(loc='lower right')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tI1_3oJnx1e"
      },
      "outputs": [],
      "source": [
        "def calculate_class_accuracy(obj = None, confusion_matrix_ann = None):\n",
        "    each_model_each_class_accuracy = {}\n",
        "    res = None\n",
        "    if obj is not None:\n",
        "        for model in obj.model_names:\n",
        "            confusion_matrix = obj.get_metric_scores(model)['Test Confusion Matrix']\n",
        "            num_classes = len(confusion_matrix)\n",
        "            class_accuracies = {}\n",
        "\n",
        "            for i in range(num_classes):\n",
        "                TP = confusion_matrix[i, i]\n",
        "                FP = sum(confusion_matrix[:, i]) - TP\n",
        "                FN = sum(confusion_matrix[i, :]) - TP\n",
        "                TN = np.sum(confusion_matrix) - TP - FP - FN\n",
        "\n",
        "                total_samples = TP + TN + FP + FN\n",
        "\n",
        "                accuracy = (TP + TN) / total_samples\n",
        "                class_accuracies[f'Class {i}'] = {'Accuracy (%)': round(accuracy * 100.00, 2), 'Total Samples': total_samples, 'Total Correct Samples Predicted': TP + TN}\n",
        "            each_model_each_class_accuracy[model] = class_accuracies\n",
        "\n",
        "    if confusion_matrix_ann is not None:\n",
        "        num_classes = len(confusion_matrix_ann)\n",
        "        class_accuracies = {}\n",
        "        for i in range(num_classes):\n",
        "            TP = confusion_matrix_ann[i, i]\n",
        "            FP = sum(confusion_matrix_ann[:, i]) - TP\n",
        "            FN = sum(confusion_matrix_ann[i, :]) - TP\n",
        "            TN = np.sum(confusion_matrix_ann) - TP - FP - FN\n",
        "\n",
        "            total_samples = TP + TN + FP + FN\n",
        "\n",
        "            accuracy = (TP + TN) / total_samples\n",
        "            class_accuracies[f'Class {i}'] = {'Accuracy (%)': round(accuracy * 100.00, 2), 'Total Samples': total_samples, 'Total Correct Samples Predicted': TP + TN}\n",
        "        each_model_each_class_accuracy['ANN'] = class_accuracies\n",
        "\n",
        "    for model_name in each_model_each_class_accuracy:\n",
        "        tmp = pd.DataFrame(each_model_each_class_accuracy[model_name]).T.stack(0).reset_index().rename(columns = {'level_0': 'Class', 'level_1': 'Attributes', 0: model_name}).set_index(['Class', 'Attributes'], drop = True)\n",
        "        if res is None:\n",
        "            res = tmp\n",
        "        else:\n",
        "            res = res.join(tmp)\n",
        "\n",
        "    return res\n",
        "\n",
        "with open(\"/content/hypertuned_models.pkl\", \"rb\") as file:\n",
        "    tmp1 = joblib.load(file)\n",
        "\n",
        "tmp2 = evaluate_ann(torch.load(\"/content/ann_models.pkl\"),\n",
        "                    *train_test_split(x, y, test_size=0.3, random_state=42))[0]['Test Confusion Matrix']\n",
        "\n",
        "class_accuracy_df = calculate_class_accuracy(tmp1, tmp2).query(\"Attributes == 'Accuracy (%)'\")\n",
        "\n",
        "data_to_plot = class_accuracy_df[['XGB','LGBM', 'DecisionTree', 'RandomForest', 'GradientBoosting', 'MLP Neural Net']]\n",
        "\n",
        "ax = data_to_plot.plot(marker='o', figsize=(10, 6))\n",
        "\n",
        "# for model in data_to_plot.columns:\n",
        "#     for index, value in enumerate(data_to_plot[model]):\n",
        "#         ax.annotate(f'{value:.2f}%', (index, value), textcoords=\"offset points\", xytext=(0, 5), ha='center')\n",
        "\n",
        "new_labels = data_to_plot.index.get_level_values(0).to_list()\n",
        "ax.set_xticks(range(len(new_labels)))\n",
        "ax.set_xticklabels(new_labels)\n",
        "\n",
        "ax.set_xlabel('Target', fontsize=15)\n",
        "ax.set_ylabel('Accuracy (%)', fontsize=15)\n",
        "# ax.set_title('Occupancy Levels Accuracy Comparison', fontsize=15)\n",
        "ax.tick_params(axis='y', which='both', labelsize=15);\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK36rbgVzxWn"
      },
      "source": [
        "## Each Class Accuracy of Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RN_FA4jNH6Zh"
      },
      "outputs": [],
      "source": [
        "def calculate_class_metrics(obj = None, confusion_matrix_ann = None):\n",
        "    each_model_each_class_metrics = {}\n",
        "    res = None\n",
        "    epsilon = 1e-7  # small constant\n",
        "\n",
        "    if obj is not None:\n",
        "        for model in obj.model_names:\n",
        "            confusion_matrix = obj.get_metric_scores(model)['Test Confusion Matrix']\n",
        "            num_classes = len(confusion_matrix)\n",
        "            class_metrics = {}\n",
        "\n",
        "            for i in range(num_classes):\n",
        "                TP = confusion_matrix[i, i]\n",
        "                FP = sum(confusion_matrix[:, i]) - TP\n",
        "                FN = sum(confusion_matrix[i, :]) - TP\n",
        "                TN = np.sum(confusion_matrix) - TP - FP - FN\n",
        "\n",
        "                total_samples = TP + FP + FN+TN\n",
        "\n",
        "                accuracy = TP / (total_samples + epsilon)\n",
        "                precision = TP / (TP + FP + epsilon)\n",
        "                recall = TP / (TP + FN + epsilon)\n",
        "                f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
        "\n",
        "                class_metrics[f'Class {i}'] = {'Accuracy (%)': round(accuracy * 100.00, 2), 'Precision (%)': round(precision * 100.00, 2), 'Recall (%)': round(recall * 100.00, 2), 'F1 Score (%)': round(f1_score * 100.00, 2)}\n",
        "            each_model_each_class_metrics[model] = class_metrics\n",
        "\n",
        "    if confusion_matrix_ann is not None:\n",
        "        num_classes = len(confusion_matrix_ann)\n",
        "        class_metrics = {}\n",
        "        for i in range(num_classes):\n",
        "            TP = confusion_matrix_ann[i, i]\n",
        "            FP = sum(confusion_matrix_ann[:, i]) - TP\n",
        "            FN = sum(confusion_matrix_ann[i, :]) - TP\n",
        "            TN = np.sum(confusion_matrix) - TP - FP - FN\n",
        "\n",
        "            total_samples = TP + FP + FN + TN\n",
        "\n",
        "            accuracy = TP / (total_samples + epsilon)\n",
        "            precision = TP / (TP + FP + epsilon)\n",
        "            recall = TP / (TP + FN + epsilon)\n",
        "            f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
        "\n",
        "            class_metrics[f'Class {i}'] = {'Accuracy (%)': round(accuracy * 100.00, 2), 'Precision (%)': round(precision * 100.00, 2), 'Recall (%)': round(recall * 100.00, 2), 'F1 Score (%)': round(f1_score * 100.00, 2)}\n",
        "        each_model_each_class_metrics['ANN'] = class_metrics\n",
        "\n",
        "    for model_name in each_model_each_class_metrics:\n",
        "        tmp = pd.DataFrame(each_model_each_class_metrics[model_name]).T.stack(0).reset_index().rename(columns = {'level_0': 'Class', 'level_1': 'Attributes', 0: model_name}).set_index(['Class', 'Attributes'], drop = True)\n",
        "        if res is None:\n",
        "            res = tmp\n",
        "        else:\n",
        "            res = res.join(tmp, how='outer')\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pdp8lyjoe4r"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/hypertuned_models.pkl\", \"rb\") as file:\n",
        "    tmp1 = joblib.load(file)\n",
        "\n",
        "tmp2 = evaluate_ann(torch.load(\"/content/ann_models.pkl\"),\n",
        "                    *train_test_split(x, y, test_size=0.3, random_state=42))[0]['Test Confusion Matrix']\n",
        "\n",
        "calculate_class_accuracy(tmp1, tmp2)\n",
        "# calculate_class_accuracy(tmp1, tmp2).query(\"Attributes == 'Accuracy (%)'\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}